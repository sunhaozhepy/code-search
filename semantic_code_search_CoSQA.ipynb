{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNg9ivVrt+wU8nt0sGBaXvY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## Code AST construction and preprocessing"],"metadata":{"id":"dfT3KC9ltcZU"}},{"cell_type":"markdown","source":["对于IR，我们使用最普通的retrieval方法，因为CSN的数据量尚允许；注意code search和IR（其子集是semantic search）数据类型有所不同\n","\n","IBA类似MNR loss；数据集使用CodeSearchNet（AdvTest）；数据类型不同且模型不同时，fusion layer就很重要了\n","\n","使用sentence-transformer；由于训练数据必须是text并且必须使用bi-encoder，因此只作为encoder部分（暂定text encoder不训练否则可能学习率不兼容）\n","\n","最主要比较的是GNN模型的选择，以及和baseline的比较"],"metadata":{"id":"ZzYRZd4QCj2i"}},{"cell_type":"markdown","source":["## evaluate和ModelContra还是在当黑箱用"],"metadata":{"id":"TATFoMMQ0kYZ"}},{"cell_type":"code","source":["# Install torch geometric\n","!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","!pip install torch-geometric\n","!pip install transformers\n","!pip install -U sentence-transformers"],"metadata":{"id":"uJQA1QKj2zpp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681291405742,"user_tz":-480,"elapsed":2881844,"user":{"displayName":"Haozhe Sun","userId":"05710037291064701493"}},"outputId":"03b5b49b-eec5-4ab0-d3e0-dbd0fa844f80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","Collecting torch-scatter\n","  Downloading torch_scatter-2.1.1.tar.gz (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: torch-scatter\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-scatter: filename=torch_scatter-2.1.1-cp39-cp39-linux_x86_64.whl size=3518810 sha256=7a1fe66a856977236b8be995f41d4dd5fde0b8443df4396ebc7a513230034c17\n","  Stored in directory: /root/.cache/pip/wheels/d5/0c/18/11b4cf31446c5d460543b0fff930fcac3a3f8a785e5c73fb15\n","Successfully built torch-scatter\n","Installing collected packages: torch-scatter\n","Successfully installed torch-scatter-2.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","Collecting torch-sparse\n","  Downloading torch_sparse-0.6.17.tar.gz (209 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n","Building wheels for collected packages: torch-sparse\n","  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-sparse: filename=torch_sparse-0.6.17-cp39-cp39-linux_x86_64.whl size=2748123 sha256=4ac011c7d4fc2f49679ab62b3f55d4d1d43671be04ba5473d63a4d17adf66fae\n","  Stored in directory: /root/.cache/pip/wheels/f8/43/54/bcb8acdd1109bd1e4c71106747af298d0315cdf3f090b2ae43\n","Successfully built torch-sparse\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.17\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.3.0.tar.gz (616 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.3.0-py3-none-any.whl size=909897 sha256=155bd22ec841bf0a3dd48f479d02b34187a04ae7cd6472d4a24418f5e9f978b6\n","  Stored in directory: /root/.cache/pip/wheels/cd/7d/6b/17150450b80b4a3656a84330e22709ccd8dc0f8f4773ba4133\n","Successfully built torch-geometric\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.27.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=af7c7e7bfe43d399177c25b2e409bc346f0b40a7e6493689e7b62d42392f4a4b\n","  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n","Successfully built sentence-transformers\n","Installing collected packages: sentencepiece, sentence-transformers\n","Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.97\n"]}]},{"cell_type":"code","source":["# https://docs.python.org/3/library/ast.html#ast-helpers\n","# some of the classes may vary according to the version of Python, e.g. \"Match\" is not present before 3.10\n","\n","class_list = [\n","  \"AST\",\n","  \"Constant\", \"FormattedValue\", \"JoinedStr\",\n","  \"List\", \"Tuple\", \"Set\", \"Dict\", \"Name\",\n","  \"Load\", \"Store\", \"Del\", \"Starred\", \"Expr\",\n","  \"UnaryOp\", \"UAdd\", \"USub\", \"Not\", \"Invert\",\n","  \"BinOp\", \"Add\", \"Sub\", \"Mult\", \"Div\", \"FloorDiv\",\n","  \"Mod\", \"Pow\", \"LShift\", \"RShift\", \"BitOr\",\n","  \"BitXor\", \"BitAnd\", \"MatMult\", \"BoolOp\",\n","  \"And\", \"Or\", \"Compare\", \"Eq\", \"NotEq\",\n","  \"Lt\", \"LtE\", \"Gt\", \"GtE\", \"Is\", \"IsNot\",\n","  \"In\", \"NotIn\", \"Call\", \"keyword\", \"IfExp\",\n","  \"Attribute\", \"NamedExpr\", \"Subscript\", \"Slice\",\n","  \"ListComp\", \"SetComp\", \"GeneratorExp\", \"DictComp\",\n","  \"comprehension\", \"Assign\", \"AnnAssign\", \"AugAssign\",\n","  \"Raise\", \"Assert\", \"Delete\", \"Pass\", \"Import\",\n","  \"ImportFrom\", \"alias\", \"If\", \"For\", \"While\",\n","  \"Break\", \"Continue\", \"Try\", \"TryStar\", \"ExceptHandler\",\n","  \"With\", \"withitem\", \"Match\", \"match_case\",\n","  \"MatchValue\", \"MatchSingleton\", \"MatchSequence\",\n","  \"MatchStar\", \"MatchMapping\", \"MatchClass\",\n","  \"MatchAs\", \"MatchOr\", \"FunctionDef\", \"Lambda\",\n","  \"arguments\", \"arg\", \"Return\", \"Yield\", \"YieldFrom\",\n","  \"Global\", \"Nonlocal\", \"ClassDef\", \"AsyncFunctionDef\",\n","  \"Await\", \"AsyncFor\", \"AsyncWith\"\n","]\n","\n","class_dict = {class_list[i]: i for i in range(len(class_list))}"],"metadata":{"id":"aFT8_sYDWOlk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bk7oWl_LfpUg"},"outputs":[],"source":["# based on https://gist.github.com/joshmarlow/4001898\n","# AST到NetworkX的转化，NetworkX到PyG图的转化，以及节点特征的构建\n","import ast\n","import networkx as nx\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","class viz_walker(ast.NodeVisitor):\n","  def __init__(self):\n","    self.stack = []\n","    self.graph = nx.Graph()\n","    self.vectorizer = CountVectorizer(lowercase=False, vocabulary=class_dict, binary=True)\n","\n","  def generic_visit(self, stmt):\n","    node_name = str(stmt)\n","\n","    parent_name = None\n","\n","    if self.stack:\n","      parent_name = self.stack[-1]\n","\n","    self.stack.append(node_name)\n","\n","    self.graph.add_node(node_name, x=self.vectorizer.transform([stmt.__class__.__name__]).toarray().squeeze())\n","\n","    if parent_name:\n","      self.graph.add_edge(node_name, parent_name)\n","\n","    super(self.__class__, self).generic_visit(stmt)\n","\n","    self.stack.pop()"]},{"cell_type":"markdown","source":["## GNN model as code encoder"],"metadata":{"id":"feEYAfhPtgzk"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import torch.nn.functional as F\n","print(torch.__version__)\n","\n","import os\n","\n","from torch_geometric.nn import GCNConv, global_mean_pool\n","from torch_geometric.utils import from_networkx"],"metadata":{"id":"H8eg65gE1Rla","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681291435428,"user_tz":-480,"elapsed":3,"user":{"displayName":"Haozhe Sun","userId":"05710037291064701493"}},"outputId":"707b375f-46b2-473c-deaf-93e4949af2f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.0+cu118\n"]}]},{"cell_type":"code","source":["# based on codebase of CS224W\n","class GCN(nn.Module): # 之后拓展到更多模型\n","  def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout, return_embeds=False):\n","    super(GCN, self).__init__()\n","\n","    # A list of GCNConv layers\n","    self.convs = nn.ModuleList()\n","\n","    # A list of 1D batch normalization layers\n","    self.bns = nn.ModuleList()\n","\n","    # The log softmax layer\n","    self.softmax = nn.LogSoftmax(dim=-1)\n","\n","    self.convs.append(GCNConv(input_dim, hidden_dim))\n","    for _ in range(num_layers - 2):\n","      self.convs.append(GCNConv(hidden_dim, hidden_dim))\n","    self.convs.append(GCNConv(hidden_dim, output_dim))\n","\n","    for _ in range(num_layers - 1):\n","      self.bns.append(nn.BatchNorm1d(hidden_dim))\n","\n","    self.relu = nn.ReLU()\n","\n","    # Probability of an element getting zeroed\n","    self.dropout = nn.Dropout(p=dropout)\n","\n","    # Skip classification layer and return node embeddings\n","    self.return_embeds = return_embeds\n","\n","  def reset_parameters(self):\n","    for conv in self.convs:\n","      conv.reset_parameters()\n","    for bn in self.bns:\n","      bn.reset_parameters()\n","\n","  def forward(self, x, adj_t):\n","    for i in range(len(self.convs) - 1):\n","      x = self.convs[i](x, adj_t) # 可以是edge list也可以是adjacency matrix\n","      x = self.bns[i](x)\n","      x = self.relu(x)\n","      x = self.dropout(x)\n","\n","    out = self.convs[-1](x, adj_t) # 如果输出embedding，那么维度是output_dim\n","    if self.return_embeds == False:\n","      out = self.softmax(out)\n","\n","    return out"],"metadata":{"id":"LMhNbozL1YMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### GCN to predict graph property\n","# based on codebase of CS224W\n","class GCN_Graph(nn.Module):\n","  def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n","    super(GCN_Graph, self).__init__()\n","\n","    # Node embedding model\n","    self.gnn_node = GCN(input_dim, hidden_dim,\n","      hidden_dim, num_layers, dropout, return_embeds=True)\n","\n","    self.pool = global_mean_pool # 从节点embedding出发，用mean pooling得到graph embedding\n","\n","  def reset_parameters(self):\n","    self.gnn_node.reset_parameters()\n","    self.linear.reset_parameters()\n","\n","  def forward(self, batched_data):\n","    x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n","\n","    out = self.gnn_node(x, edge_index)\n","    out = self.pool(out, batch)\n","\n","    return out"],"metadata":{"id":"MLk35ioO13Bv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The whole retrieval model"],"metadata":{"id":"oD_DO94eYLJz"}},{"cell_type":"code","source":["class ModelContra(nn.Module):\n","  def __init__(self, text_encoder, code_encoder):\n","    super(ModelContra, self).__init__()\n","    self.text_encoder = text_encoder\n","    self.code_encoder = code_encoder\n","    self.mlp = nn.Sequential(nn.Linear(768*4, 768),\n","      nn.Tanh(),\n","      nn.Linear(768, 1),\n","      nn.Sigmoid())\n","    self.loss_func = nn.BCELoss()\n","\n","  def forward(self, code_inputs, nl_inputs, labels, return_vec=False):\n","    # nl_inputs是text的list，code_inputs是PyG Data的list\n","    nl_vec = self.text_encoder.encode(nl_inputs, convert_to_tensor=True)\n","    code_vec = self.code_encoder(code_inputs)\n","    if return_vec:\n","      return code_vec, nl_vec\n","\n","    nl_vec = nl_vec.unsqueeze(1).repeat([1, 64, 1])\n","    code_vec = code_vec.unsqueeze(0).repeat([64, 1, 1])\n","    logits = self.mlp(torch.cat((nl_vec, code_vec, nl_vec-code_vec, nl_vec*code_vec), 2)).squeeze(2) # (Batch, Batch)\n","    matrix_labels = torch.diag(labels).float()  # (Batch, Batch)\n","    poss = logits[matrix_labels==1]\n","    negs = logits[matrix_labels==0]\n","\n","    # loss = self.loss_func(logits, matrix_labels)\n","    # bce equals to -(torch.log(1-logits[matrix_labels==0]).sum() + torch.log(logits[matrix_labels==1]).sum()) / (bs*bs)\n","    loss = - (torch.log(1 - negs).mean() + torch.log(poss).mean())\n","    predictions = (logits.gather(0, torch.arange(64, device=loss.device).unsqueeze(0)).squeeze(0) > 0.5).int()\n","    return loss, predictions"],"metadata":{"id":"wMNcOyMoYKoz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data and model preparation"],"metadata":{"id":"dsuZA35sTVdK"}},{"cell_type":"code","source":["import random\n","\n","import numpy as np\n","from torch.utils.data import Dataset\n","\n","from torch_geometric.loader import DataLoader\n","\n","import json\n","\n","from tqdm import tqdm, trange\n","\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from sentence_transformers import SentenceTransformer"],"metadata":{"id":"PDioCQuYPsLh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InputFeatures(object):\n","  def __init__(self, code_data, query, label, idx):\n","    # Here code_data is a PyG Data ovject\n","    self:code_data = code_data\n","    self.query = query\n","    self.label = label\n","    self.idx = idx\n","\n","\n","def convert_examples_to_features(js):\n","  label = js['label']\n","\n","  code = js['code']\n","  parsed = ast.parse(code)\n","  nw = viz_walker()\n","  nw.visit(parsed)\n","  code_data = from_networkx(nw.graph)\n","\n","  nl = js['doc']\n","\n","  return InputFeatures(code_data, nl, label, js['idx'])\n","\n","\n","class TextDataset(Dataset):\n","  def __init__(self, data_path=None):\n","    # file：json文件，每一个dict中包括：idx, query, doc, code(或者叫function_tokens，list形式), docstring_tokens(list形式)\n","    self.examples = []\n","    self.data = []\n","    with open(data_path, 'r') as f:\n","      self.data = json.load(f)\n","    for js in self.data:\n","      self.examples.append(convert_examples_to_features(js))\n","\n","  def __len__(self):\n","    return len(self.examples)\n","\n","  def __getitem__(self, i):\n","    return self.examples[i].code, self.examples[i].query, self.examples[i].label\n","\n","\n","class RetrievalDataset(Dataset):\n","  def __init__(self, retrieval_code_base, data_path=None):\n","    self.codes = []\n","    self.data = []\n","    self.examples = []  # codebase用code和code当成6k pair; testdata用query和code当成pair来算; 分别是examples的0-6267和6267-6767\n","    code_file = retrieval_code_base\n","    data_file = data_path\n","\n","    with open(code_file, 'r') as f:\n","      self.codes = json.loads(f.read())\n","    for code, code_id in self.codes.items():\n","      js = {'code': code, 'doc': code, 'label': 1, 'idx': code_id}\n","      self.examples.append(convert_examples_to_features(js))\n","\n","    with open(data_file, 'r') as f:\n","      self.data = json.load(f)\n","    for js in self.data:\n","      new_js = {'code': js['code'], 'doc': js['doc'], 'label': js['label'], 'idx': js['retrieval_idx']}\n","      self.examples.append(convert_examples_to_features(new_js))\n","\n","  def __len__(self):\n","    return len(self.examples)\n","\n","  def __getitem__(self, i):\n","    return self.examples[i].code, self.examples[i].query, self.examples[i].label\n","\n","\n","def set_seed(seed=45):\n","  random.seed(seed)\n","  os.environ['PYHTONHASHSEED'] = str(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed(seed)\n","  torch.backends.cudnn.deterministic = True"],"metadata":{"id":"tlDdD-LGht14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model):\n","    print(\"Running evaluation...\")\n","    model.eval()\n","    all_first_vec = []\n","    all_second_vec = []\n","    for batch in eval_dataloader:\n","        code_inputs = batch[0].to(device)\n","        nl_inputs = batch[1]\n","        labels = batch[2].to(device)\n","        with torch.no_grad():\n","            code_vec, nl_vec = model(code_inputs, nl_inputs, labels, return_vec=True)\n","            all_first_vec.append(code_vec.cpu())\n","            all_second_vec.append(nl_vec.cpu())\n","    code_vectors = torch.cat(all_first_vec, 0).squeeze()[:6267, :]\n","    query_vectors = torch.cat(all_second_vec, 0).squeeze()[6267:, :]\n","    assert code_vectors.size(0) == 6267\n","    assert query_vectors.size(0) == 500\n","\n","    repeat_nl_vec = query_vectors.unsqueeze(1).to(device)\n","    scores = []\n","    with torch.no_grad():\n","        for idx in trange(6267):\n","            repeat_code_vec = code_vectors[idx, :].unsqueeze(0).repeat([500, 1, 1]).to(device)\n","            logits = model.mlp(torch.cat((repeat_nl_vec, repeat_code_vec, repeat_nl_vec - repeat_code_vec, repeat_nl_vec * repeat_code_vec),2)).squeeze(2)\n","            scores.append(logits.cpu())\n","\n","    scores = torch.cat(scores, 1)\n","    results = []\n","    mrr = 0\n","    for idx in range(len(scores)):\n","        rank = torch.argsort(-scores[idx]).tolist()\n","        example = eval_dataset.examples[idx+6267]\n","        item = {}\n","        item['ans'] = example.idx\n","        item['rank'] = rank\n","        item['rr'] = 1 / (rank.index(example.idx)+1)\n","        mrr += item['rr']\n","        results.append(item)\n","    mrr = mrr / len(scores)\n","    return mrr"],"metadata":{"id":"x5OHCZLwmbDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","sentence-transformer的保存问题\n","需要去掉不能被parse的code\n","\"\"\"\n","device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n","print(f\"using {device}.\")\n","\n","train_data_path = \"./cosqa-retrieval-train-19604-qra-switch-28624.json\"\n","eval_data_path = \"./cosqa-retrieval-dev-500.json\"\n","retrieval_code_base = \"./code_idx_map.txt\"\n","output_dir = \"./outputs\"\n","\n","# set_seed(45)\n","\n","text_encoder = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n","code_encoder = GCN_Graph(input_dim=len(class_dict), hidden_dim=768, num_layers=3, dropout=0.5)\n","model = ModelContra(text_encoder, code_encoder)\n","model.to(device)\n","\n","for param in text_encoder.parameters(): # we first test with fixed text encoder\n","  param.requires_grad = False\n","\n","train_dataset = TextDataset(train_data_path)\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64)\n","eval_dataset = RetrievalDataset(retrieval_code_base, eval_data_path)\n","eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=64)\n","\n","num_train_epochs = 5\n","save_steps = 500\n","\n","optimizer = AdamW(model.parameters(), lr=0.01)\n","scheduler = get_linear_schedule_with_warmup(optimizer, len(train_dataloader), len(train_dataloader) * num_train_epochs)\n","\n","step = 0\n","best_mrr = 0.0\n","\n","for idx in range(num_train_epochs):\n","  for batch in tqdm(train_dataloader):\n","    code_inputs = batch[0].to(device)\n","    nl_inputs = batch[1]\n","    labels = batch[2].to(device)\n","\n","    model.train()\n","    optimizer.zero_grad()\n","    loss, predictions = model(code_inputs, nl_inputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","    step += 1\n","\n","    if step % save_steps == 0:\n","      mrr = evaluate(model)\n","      print(f\"Mrr = {mrr}\")\n","      if mrr >= best_mrr:\n","        best_mrr = mrr\n","        print(f\"Save checkpoint!\")\n","\n","        checkpoint_prefix = 'checkpoint-best-mrr'\n","        output_dir = os.path.join(output_dir, checkpoint_prefix)\n","        if not os.path.exists(output_dir):\n","          os.makedirs(output_dir)\n","        model_to_save = model.module if hasattr(model, 'module') else model\n","\n","        torch.save(model_to_save.state_dict(), os.path.join(output_dir, 'pytorch_model.bin'))"],"metadata":{"id":"B4dZIzBDQyVJ","colab":{"base_uri":"https://localhost:8080/","height":431},"executionInfo":{"status":"error","timestamp":1681291593850,"user_tz":-480,"elapsed":2845,"user":{"displayName":"Haozhe Sun","userId":"05710037291064701493"}},"outputId":"5aad70a4-ac78-4988-cf9d-e100f200f062"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["using 0.\n"]},{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n","  File \u001b[1;32m\"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \u001b[1;32m\"<ipython-input-22-5c3f166ab173>\"\u001b[0m, line \u001b[1;32m23\u001b[0m, in \u001b[1;35m<cell line: 23>\u001b[0m\n    train_dataset = TextDataset(train_data_path)\n","  File \u001b[1;32m\"<ipython-input-19-1d0db112212d>\"\u001b[0m, line \u001b[1;32m32\u001b[0m, in \u001b[1;35m__init__\u001b[0m\n    self.examples.append(convert_examples_to_features(js))\n","  File \u001b[1;32m\"<ipython-input-19-1d0db112212d>\"\u001b[0m, line \u001b[1;32m14\u001b[0m, in \u001b[1;35mconvert_examples_to_features\u001b[0m\n    parsed = ast.parse(code)\n","\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.9/ast.py\"\u001b[0;36m, line \u001b[0;32m50\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0m\n","\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    except EnvironmentError, e:\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n","print(f\"using {device}.\")\n"," \n","test_data_file = \"./cosqa-retrieval-test-500.json\"\n","retrieval_code_base = \"./code_idx_map.txt\"\n","retrieval_predictions_output = \"./outputs/retrieval_outputs.txt\"\n","output_dir = \"./outputs\"\n","\n","# set_seed(45)\n","\n","text_encoder = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n","code_encoder = GCN_Graph(input_dim=len(class_dict), hidden_dim=768, num_layers=3, dropout=0.5)\n","model = ModelContra(text_encoder, code_encoder)\n","model.load_state_dict(torch.load(os.path.join(output_dir, 'checkpoint-best-mrr/pytorch_model.bin')))\n","model.to(device)\n","\n","test_dataset = RetrievalDataset(retrieval_code_base, test_data_file)\n","test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n","\n","print(\"Running Test...\")\n","model.eval()\n","all_first_vec = []\n","all_second_vec = []\n","for batch in test_dataloader:\n","    code_inputs = batch[0].to(device)\n","    nl_inputs = batch[1]\n","    labels = batch[2].to(device)\n","    with torch.no_grad():\n","        code_vec, nl_vec = model(code_inputs, nl_inputs, labels, return_vec=True)\n","        all_first_vec.append(code_vec.cpu())\n","        all_second_vec.append(nl_vec.cpu())\n","code_vectors = torch.cat(all_first_vec, 0).squeeze()[:6267, :]\n","query_vectors = torch.cat(all_second_vec, 0).squeeze()[6267:, :]\n","assert code_vectors.size(0) == 6267\n","assert query_vectors.size(0) == 500\n","\n","repeat_nl_vec = query_vectors.unsqueeze(1).to(device)\n","scores = []\n","with torch.no_grad():\n","    for idx in trange(6267):\n","        repeat_code_vec = code_vectors[idx, :].unsqueeze(0).repeat([500, 1, 1]).to(device)\n","        logits = model.mlp(torch.cat((repeat_nl_vec, repeat_code_vec, repeat_nl_vec - repeat_code_vec, repeat_nl_vec * repeat_code_vec),2)).squeeze(2)\n","        scores.append(logits.cpu())\n","\n","scores = torch.cat(scores, 1)\n","results = []\n","mrr = 0\n","for idx in range(len(scores)):\n","    rank = torch.argsort(-scores[idx]).tolist()\n","    example = test_dataset.examples[idx+6267]\n","    item = {}\n","    item['ans'] = example.idx\n","    item['rank'] = rank\n","    item['rr'] = 1 / (rank.index(example.idx)+1)\n","    mrr += item['rr']\n","    results.append(item)\n","mrr = mrr / len(scores)\n","print(f\"Final test MRR: {mrr}\")\n","with open(retrieval_predictions_output, 'w') as f:\n","    json.dump(results, f, indent=4)"],"metadata":{"id":"sA8H6_a7pAY0"},"execution_count":null,"outputs":[]}]}